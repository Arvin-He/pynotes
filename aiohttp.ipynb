{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asyncio与Aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\codeop.py:133: RuntimeWarning: coroutine 'get_resp' was never awaited\n",
      "  codeob = compile(source, filename, symbol, self.flags, 1)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Event loop is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c0769e73243e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_resp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-c0769e73243e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_resp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mcreate_task\u001b[1;34m(self, coro)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtask\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_factory\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mtask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_closed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Event loop is closed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_asyncgen_finalizer_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Event loop is closed"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import logging.handlers\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import cchardet\n",
    "import async_timeout\n",
    "from html import unescape\n",
    "from fnmatch import fnmatch\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Accept-Encoding': 'gzip, deflate'\n",
    "}\n",
    "\n",
    "# MONGODB参数\n",
    "MONGODB = {\n",
    "    \"user\": \"xxx\",\n",
    "    \"passwd\": \"xxx\",\n",
    "    \"host\": \"127.0.0.1:27017\",\n",
    "    \"dbname\": \"xxx\"\n",
    "}\n",
    "\n",
    "def init_root_logger_settings(log_name='spiders', logConsole=True):\n",
    "    LOG_FORMAT = \"%(asctime)s [%(levelname)s] [%(filename)s] [%(lineno)d]: %(message)s\"\n",
    "    log_dir = os.path.join(os.path.dirname(\n",
    "        os.path.dirname(os.path.abspath(__file__))), \"logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "    fh = logging.handlers.TimedRotatingFileHandler(filename=os.path.join(log_dir, log_name),\n",
    "                                                   when='midnight', interval=1, encoding='utf-8')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.suffix = \"%Y-%m-%d.log\"\n",
    "    fh.setFormatter(formatter)\n",
    "    root_logger.addHandler(fh)\n",
    "\n",
    "    if logConsole:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        root_logger.addHandler(ch)\n",
    "\n",
    "def connect_mongo(MONGODB):\n",
    "    client = MongoClient(\n",
    "        'mongodb://{}:{}@{}/{}'.format(MONGODB['user'],\n",
    "                                       MONGODB['passwd'],\n",
    "                                       MONGODB['host'],\n",
    "                                       MONGODB['dbname']))\n",
    "    return client[MONGODB['dbname']]\n",
    "\n",
    "class Spider(object):\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "        self.parser = Parser(params=params)\n",
    "\n",
    "    def run(self):\n",
    "        logging.info('Spider started!')\n",
    "        start_time = datetime.now()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        try:\n",
    "            semaphore = asyncio.Semaphore(self.params.get('concurrency', 5))\n",
    "            # tasks = asyncio.wait(self.parser.task(semaphore))\n",
    "            loop.run_until_complete(self.parser.init_parse(semaphore))\n",
    "            loop.run_until_complete(self.parser.task(loop, semaphore))\n",
    "        except KeyboardInterrupt:\n",
    "            for task in asyncio.Task.all_tasks():\n",
    "                task.cancel()\n",
    "            loop.run_forever()\n",
    "        finally:\n",
    "            end_time = datetime.now()\n",
    "            # logging.info('Requests count: {}'.format(self.urls_count))\n",
    "            # logging.info('Error count: {}'.format(len(self.error_urls)))\n",
    "            logging.info('Time usage: {}'.format(end_time - start_time))\n",
    "            logging.info('Spider finished!')\n",
    "            loop.close()\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    \"\"\"解析url\"\"\"\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "        self.item = Item(params=params)\n",
    "        # 存放url的队列\n",
    "        self.urlsQ = asyncio.Queue(maxsize=self.params.get('queue_size', 500))\n",
    "        # 存放response的队列\n",
    "        self.respQ = asyncio.Queue(maxsize=self.params.get('queue_size', 500))\n",
    "        self.filter_urls = set()\n",
    "        self.error_urls = set()\n",
    "        self.urls_count = 0\n",
    "\n",
    "    def is_running(self):\n",
    "        is_running = False\n",
    "        if not self.urlsQ.empty() or not self.respQ.empty():\n",
    "            is_running = True\n",
    "        return is_running\n",
    "\n",
    "    async def init_parse(self, semaphore):\n",
    "        async with aiohttp.ClientSession(cookies=self.params.get('cookies')) as session:\n",
    "            logging.info('init first parse...')\n",
    "            resp = await self.fetch(self.URL(self.params.get('start_url')), session, semaphore)\n",
    "            await self.parse_urls(resp)\n",
    "\n",
    "    def URL(self, url, params=None):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        return {'url': url,\n",
    "                'base_url': params.get('url', ''),\n",
    "                'data': params.get('data'),\n",
    "                'method': params.get('method', 'GET'),\n",
    "                'depth': params.get('depth', 0) + 1,\n",
    "                'end_type': self.params.get('end_type')}\n",
    "\n",
    "    def normal_url(self, url, base_url):\n",
    "        new_url = unescape(url.strip())\n",
    "        if not re.match('(http|https)://', new_url):\n",
    "            new_url = urljoin(base_url, new_url)\n",
    "        return new_url[:-1] if new_url.endswith('/') else new_url\n",
    "\n",
    "    async def do_with_302(self, resp):\n",
    "        # 处理302重定向漏掉的页面\n",
    "        for response in resp.history:\n",
    "            await self.respQ.put_nowait(response)\n",
    "            await self.item.save(response)\n",
    "\n",
    "    async def parse_urls(self, resp):\n",
    "        \"\"\"提取urls\"\"\"\n",
    "        logging.info('start parse urls...')\n",
    "        if isinstance(resp, dict):\n",
    "            base_url = resp['url']\n",
    "            soup = BeautifulSoup(resp['text'], 'lxml')\n",
    "            tags = soup.find_all(True)\n",
    "            for tag in tags:\n",
    "                params = {'method': 'GET', 'data': None, 'base_url': base_url}\n",
    "                if tag.name == 'form':\n",
    "                    url = tag.get('action', '')\n",
    "                    params['method'] = tag.get('method')\n",
    "                    params['data'] = self.parse_form_data(tag)\n",
    "                elif tag.name == 'script':\n",
    "                    url = tag.get('src', '')\n",
    "                else:\n",
    "                    url = tag.get('href', '')\n",
    "                if url and url != base_url:\n",
    "                    self.add(url, params)\n",
    "        else:\n",
    "            logging.warning('response is none.')\n",
    "            return await resp\n",
    "\n",
    "    def parse_form_data(self, tag):\n",
    "        data = {}\n",
    "        for input in tag.find_all('input'):\n",
    "            name = input.get('name')\n",
    "            if name and input.get('type') in ['text', 'password']:\n",
    "                data[name] = input.get('value', '')\n",
    "            elif input.get('type') == 'submit':\n",
    "                name = 'submit'\n",
    "                data[name] = input.get('value', '')\n",
    "            else:\n",
    "                if data.get(name) is None:\n",
    "                    data[name] = list(input.get('value', ''))\n",
    "                else:\n",
    "                    data[name].append(input.get('value', ''))\n",
    "        return data\n",
    "\n",
    "    def add(self, url, params=None):\n",
    "        # 格式化url\n",
    "        new_url = self.normal_url(url, params.get('base_url'))\n",
    "        # 根据域名列表过滤url和过滤重复url\n",
    "        if not self.url_need_filter(new_url):\n",
    "            url_obj = self.URL(new_url, params)\n",
    "            self.urlsQ.put_nowait(url_obj)\n",
    "\n",
    "    def url_need_filter(self, url):\n",
    "        # 根据域名列表过滤url,如果域名列表为空则不过滤,否则根据列表来过滤\n",
    "        if len(self.params.get('allowed_domains', [])) == 0:\n",
    "            logging.info('no need to filter')\n",
    "        else:\n",
    "            domain = urlparse(url).netloc\n",
    "            if domain not in self.params['allowed_domains']:\n",
    "                return True\n",
    "        # 过滤重复url\n",
    "        if url not in self.filter_urls:\n",
    "            self.filter_urls.add(url)\n",
    "            if len(self.filter_urls) > self.params.get('amount', 500):\n",
    "                return True\n",
    "            # logging.info('filter urls count: {}'.format(len(self.filter_urls)))\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    async def get_resp(self, resp):\n",
    "        response = {}\n",
    "        try:\n",
    "            response['request'] = str(resp.request_info)\n",
    "            # 统一url格式\n",
    "            response['url'] = str(resp.url.with_port(resp.url.port))\n",
    "            response['status'] = resp.status\n",
    "            response['content'] = await resp.read()\n",
    "            response['text'] = await resp.text()\n",
    "            if len(resp.history) > 0:\n",
    "                for history_item in  resp.history:\n",
    "                    await self.get_resp(history_item)\n",
    "        except Exception as e:\n",
    "            logging.error('get response error: {}'.format(e))\n",
    "        return response\n",
    "\n",
    "    async def fetch(self, url_obj, session, semaphore, retry=1):\n",
    "        # logging.info('start to fetch url_obj = {}'.format(url_obj))\n",
    "        with (await semaphore):\n",
    "            try:\n",
    "                headers = HEADER\n",
    "                proxy = self.params.get('proxy')\n",
    "                if url_obj['method'] == 'GET':\n",
    "                    async with session.get(url_obj['url'], headers=headers, proxy=proxy,\n",
    "                                           timeout=self.params.get('timeout', 10)) as resp:\n",
    "                        return await self.get_resp(resp)\n",
    "                else:\n",
    "                    async with session.post(url_obj['url'], headers=headers, proxy=proxy,\n",
    "                                            data=url_obj['data'], timeout=self.params.get('timeout', 10)) as resp:\n",
    "                        return await self.get_resp(resp)\n",
    "            except Exception as e:\n",
    "                logging.error('fetch error: {}'.format(e))\n",
    "                if retry > 0:\n",
    "                    logging.warning('fetch url failed, try twice.')\n",
    "                    await self.fetch(url_obj, session, semaphore, retry - 1)\n",
    "\n",
    "    async def execute_url(self, url_obj, session, semaphore):\n",
    "        logging.info('start execute url...')\n",
    "        resp = await self.fetch(url_obj, session, semaphore)\n",
    "        try:\n",
    "            if isinstance(resp, dict):\n",
    "                # logging.info('respQ put in a resp')\n",
    "                self.respQ.put_nowait(resp)\n",
    "                await self.item.save(resp)\n",
    "            else:\n",
    "                self.error_urls.add(url_obj['url'])\n",
    "                logging.info('Error url count: {}'.format(len(self.error_urls)))\n",
    "            # logging.info('respQ has {} resp'.format(self.respQ.qsize()))\n",
    "        except Exception as e:\n",
    "            logging.error('execute url error: {}'.format(e))\n",
    "\n",
    "        # logging.info('Parsed({}/{}): {}'.format(len(self.done_urls), len(self.filter_urls), url))\n",
    "        # else:\n",
    "        #     spider.parse(html)\n",
    "        #     logging.info('Followed({}/{}): {}'.format(len(self.done_urls), len(self.filter_urls), url))\n",
    "\n",
    "    async def produce_task(self, index, semaphore):\n",
    "        # 拿取resp,并解析提取url,并保存数据\n",
    "        # while self.is_running():\n",
    "        logging.info('produce task {} start...'.format(index))\n",
    "        while True:\n",
    "        # while self.is_running():\n",
    "            try:\n",
    "                resp = await asyncio.wait_for(self.respQ.get(), 5)\n",
    "                if resp is not None:\n",
    "                    # logging.info(resp.get('url'))\n",
    "                    asyncio.ensure_future(self.parse_urls(resp))\n",
    "            except asyncio.TimeoutError:\n",
    "                logging.error('produce task time out...')\n",
    "                pass\n",
    "                if not self.is_running():\n",
    "                    break\n",
    "                # await self.consume_task(semaphore)\n",
    "\n",
    "    async def consume_task(self, index, semaphore):\n",
    "        # 消费url并将response放到respQ队列中去\n",
    "        logging.info('consume task {} start...'.format(index))\n",
    "        async with aiohttp.ClientSession(cookies=self.params.get('cookies')) as session:\n",
    "            # while self.is_running():\n",
    "            while True:\n",
    "                try:\n",
    "                    url_obj = await asyncio.wait_for(self.urlsQ.get(), 5)\n",
    "                    if url_obj is not None:\n",
    "                        asyncio.ensure_future(self.execute_url(url_obj, session, semaphore))\n",
    "                except asyncio.TimeoutError:\n",
    "                    logging.error('consume task time out...')\n",
    "                    if not self.is_running():\n",
    "                        break\n",
    "                    # await self.produce_task(semaphore)\n",
    "\n",
    "    async def task(self, loop, semaphore):\n",
    "        logging.info('start to create consume and produce tasks...')\n",
    "        # while self.is_running():\n",
    "        logging.info(\"=======================\")\n",
    "        consumers = [loop.create_task(self.consume_task(i, semaphore)) for i in range(5)]\n",
    "        producers = [loop.create_task(self.produce_task(i, semaphore)) for i in range(2)]\n",
    "        await asyncio.wait(consumers + producers)\n",
    "\n",
    "\n",
    "class Item(object):\n",
    "    \"\"\"get data and save\"\"\"\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            self.params = {}\n",
    "        else:\n",
    "            self.params = params\n",
    "        db = connect_mongo(MONGODB)\n",
    "        self.movie = db['my_crawler_urls']\n",
    "        self.item_count = 0\n",
    "\n",
    "    def allow_save(self, domain):\n",
    "        # 允许域名列表为空,则全保存,不为空则过滤保存(不应在此处过滤)\n",
    "        if not self.params.get('allowed_domains'):\n",
    "            return True\n",
    "        else:\n",
    "            if domain in self.params.get('allowed_domains'):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    async def save(self, resp):\n",
    "        if resp:\n",
    "            self.movie.insert(resp)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_root_logger_settings()\n",
    "    # start_url = 'https://www.baidu.com'\n",
    "    # start_url = 'http://www.freebuf.com'\n",
    "    # start_url = 'https://passport.csdn.net/account/verify;jsessionid=38DDDAFDD2567C69A26D12D482DC090B.tomcat2'\n",
    "    params = {\n",
    "        'start_url': 'https://www.douban.com',\n",
    "        'end_type': 'PC',\n",
    "        'allowed_domains': ['www.douban.com']\n",
    "    }\n",
    "    allsiteSpider = Spider(params)\n",
    "    allsiteSpider.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Event loop is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b183ba6387fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mFuture\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mits\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \"\"\"\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\asyncio\\base_events.py\u001b[0m in \u001b[0;36m_check_closed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Event loop is closed'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_asyncgen_finalizer_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Event loop is closed"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        html = await fetch(session, 'http://python.org')\n",
    "        print(html)\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
