{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy中Item Pipelines用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当Item 在Spider中被收集之后，就会被传递到Item Pipeline中进行处理\n",
    "\n",
    "每个item pipeline组件是实现了简单的方法的python类，负责接收到item并通过它执行一些行为，\n",
    "\n",
    "同时也决定此Item是否继续通过pipeline,或者被丢弃而不再进行处理\n",
    "\n",
    "item pipeline的主要作用：\n",
    "1. 清理html数据\n",
    "2. 验证爬取的数据\n",
    "3. 去重并丢弃\n",
    "4. 将爬取的结果保存到数据库中或文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写自己的pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* process_item(self,item,spider):\n",
    "    * 每个item piple组件是一个独立的pyhton类，必须实现`process_item(self,item,spider)`方法\n",
    "    * 每个item pipeline组件都需要调用该方法，这个方法必须返回一个具有数据的dict,或者item对象，或者抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理\n",
    "\n",
    "下面的方法也可以选择实现\n",
    "\n",
    "* open_spider(self,spider):表示当spider被开启的时候调用这个方法\n",
    "\n",
    "* close_spider(self,spider):当spider关闭时这个方法被调用\n",
    "\n",
    "* from_setings(cls,settings):这个和前面说spider的时候用法是一样，用于获取settings配置文件中的信息，注意这是一个类方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些pipeline使用例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个例子是判断item中是否包含price以及price_excludes_vat，如果存在则调整price属性，让`item['price'] = item['price'] * self.vat_factor`，如果不存在则返回DropItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class PricePipeline(object):\n",
    "\n",
    "    vat_factor = 1.15\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['price']:\n",
    "            if item['price_excludes_vat']:\n",
    "                item['price'] = item['price'] * self.vat_factor\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Missing price in %s\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将item写入到json文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonWriterPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file = open('items.jl', 'wb')\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将item写入到MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时这里演示了from_crawler的用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个用于去重的过滤器，丢弃那些已经被处理过的item,假设item有一个唯一的id，但是我们spider返回的多个item中包含了相同的id,去重方法如下：这里初始化了一个集合，每次判断id是否在集合中已经存在，从而做到去重的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class DuplicatesPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ids_seen = set()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['id'] in self.ids_seen:\n",
    "            raise DropItem(\"Duplicate item found: %s\" % item)\n",
    "        else:\n",
    "            self.ids_seen.add(item['id'])\n",
    "            return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启用一个item Pipeline组件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在settings配置文件中有一个ITEM_PIPELINES的配置参数，例子如下：\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.PricePipeline': 300,\n",
    "    'myproject.pipelines.JsonWriterPipeline': 800,\n",
    "}\n",
    "```\n",
    "\n",
    "每个pipeline后面有一个数值，这个数组的范围是0-1000，这个数值确定了他们的运行顺序，数字越小越优先"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
