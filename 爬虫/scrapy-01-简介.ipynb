{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrapy-01-简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。\n",
    "\n",
    "其最初是为了 网络抓取 所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。\n",
    "\n",
    "Scrapy使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。\n",
    "\n",
    "对于会阻塞线程的操作包含访问文件、数据库或者Web、产生新的进程并需要处理新进程的输出(如运行shell命令)、执行系统层次操作的代码(如等待系统队列),Twisted提供了允许执行上面的操作但不会阻塞代码执行的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrapy项目结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在控制台输入: `scrapy startprocject project_name`, 就会在当前目录自动创建名字为projectname的scrapy项目,项目结构如下图所示:\n",
    "\n",
    "![](images/1.png)\n",
    "\n",
    "* items.py 负责数据模型的建立，类似于实体类。\n",
    "* middlewares.py 自己定义的中间件。\n",
    "* pipelines.py 负责对spider返回数据的处理。\n",
    "* settings.py 负责对整个爬虫的配置。\n",
    "* spiders目录 负责存放继承自scrapy的爬虫类。\n",
    "* scrapy.cfg scrapy基础配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### items.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "items.py里存放的是我们要爬取数据的字段信息，如下：\n",
    "\n",
    "我们分别要爬取的信息包括：\n",
    "* 文章标题\n",
    "* 文件发布时间\n",
    "* 文章url地址\n",
    "* url_object_id是我们会对地址进行md5加密\n",
    "* front_image_url 是文章下图片的url地址\n",
    "* front_image_path图片的存放路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoBoleArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    create_date = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    url_object_id = scrapy.Field()\n",
    "    front_image_url = scrapy.Field()\n",
    "    front_image_path = scrapy.Field()\n",
    "    praise_nums = scrapy.Field()\n",
    "    fav_nums = scrapy.Field()\n",
    "    comment_nums = scrapy.Field()\n",
    "    tag = scrapy.Field()\n",
    "    content = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spiders/Article.py代码分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spiders目录下的Article.py为主要的爬虫代码，包括了对页面的请求以及页面的处理，这里有几个知识点需要注意：\n",
    "\n",
    "1. 爬取的页面`http://blog.jobbole.com/all-posts/`，parse的response返回的是这个页面的信息，但是这个时候需要的是获取每个文章的地址继续访问，这里就用到了`yield Request()`这种用法，可以把获取到文章的url地址继续传递进来再次进行请求。\n",
    "\n",
    "2. scrapy提供了`response.css`这种的css选择器以及`response.xpath`的xpath选择器方法，根据具体的需求获取想要的字段信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleSpider(scrapy.Spider):\n",
    "    name = \"Article\"\n",
    "    allowed_domains = [\"blog.jobbole.com\"]\n",
    "    start_urls = ['http://blog.jobbole.com/all-posts/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        '''\n",
    "        1.获取文章列表也中具体文章url,并交给scrapy进行下载后并进行解析\n",
    "        2.获取下一页的url并交给scrapy进行下载，下载完成后，交给parse\n",
    "        :param response:\n",
    "        :return:\n",
    "        '''\n",
    "        #解析列表页中所有文章的url，并交给scrapy下载后进行解析\n",
    "        post_nodes = response.css(\"#archive .floated-thumb .post-thumb a\")\n",
    "        for post_node in post_nodes:\n",
    "            #image_url是图片的地址\n",
    "            image_url = post_node.css(\"img::attr(src)\").extract_first(\"\")\n",
    "            post_url = post_node.css(\"::attr(href)\").extract_first(\"\")\n",
    "            #这里通过meta参数将图片的url传递进来，这里用parse.urljoin的好处是如果有域名我前面的response.url不生效\n",
    "            # 如果没有就会把response.url和post_url做拼接\n",
    "            yield Request(url=parse.urljoin(response.url,post_url),meta={\"front_image_url\":parse.urljoin(response.url,image_url)},callback=self.parse_detail)\n",
    "\n",
    "        #提取下一页并交给scrapy下载\n",
    "        next_url = response.css(\".next.page-numbers::attr(href)\").extract_first(\"\")\n",
    "        if next_url:\n",
    "            yield Request(url=next_url,callback=self.parse)\n",
    "\n",
    "    def parse_detail(self,response):\n",
    "        '''\n",
    "        获取文章的详细内容\n",
    "        :param response:\n",
    "        :return:\n",
    "        '''\n",
    "        article_item = JoBoleArticleItem()\n",
    "\n",
    "        front_image_url = response.meta.get(\"front_image_url\",\"\")  #文章封面图地址\n",
    "        title = response.xpath('//div[@class=\"entry-header\"]/h1/text()').extract_first()\n",
    "\n",
    "        create_date = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/text()').extract()[0].strip().split()[0]\n",
    "\n",
    "        tag_list = response.xpath('//p[@class=\"entry-meta-hide-on-mobile\"]/a/text()').extract()\n",
    "        tag_list = [element for element in tag_list if not element.strip().endswith(\"评论\")]\n",
    "        tag =\",\".join(tag_list)\n",
    "        praise_nums = response.xpath('//span[contains(@class,\"vote-post-up\")]/h10/text()').extract()\n",
    "        if len(praise_nums) == 0:\n",
    "            praise_nums = 0\n",
    "        else:\n",
    "            praise_nums = int(praise_nums[0])\n",
    "        fav_nums  = response.xpath('//span[contains(@class,\"bookmark-btn\")]/text()').extract()[0]\n",
    "        match_re = re.match(\".*(\\d+).*\",fav_nums)\n",
    "        if match_re:\n",
    "            fav_nums = int(match_re.group(1))\n",
    "        else:\n",
    "            fav_nums = 0\n",
    "\n",
    "        comment_nums =response.xpath(\"//a[@href='#article-comment']/span/text()\").extract()[0]\n",
    "        match_com = re.match(\".*(\\d+).*\",comment_nums)\n",
    "        if match_com:\n",
    "            comment_nums = int(match_com.group(1))\n",
    "        else:\n",
    "            comment_nums=0\n",
    "\n",
    "        content = response.xpath('//div[@class=\"entry\"]').extract()[0]\n",
    "\n",
    "\n",
    "        article_item[\"url_object_id\"] = get_md5(response.url) #这里对地址进行了md5变成定长\n",
    "        article_item[\"title\"] = title\n",
    "        article_item[\"url\"] = response.url\n",
    "        try:\n",
    "            create_date = datetime.datetime.strptime(create_date,'%Y/%m/%d').date()\n",
    "        except Exception as e:\n",
    "            create_date = datetime.datetime.now().date()\n",
    "\n",
    "        article_item[\"create_date\"] = create_date\n",
    "        article_item[\"front_image_url\"] = [front_image_url]\n",
    "        article_item[\"praise_nums\"] = int(praise_nums)\n",
    "        article_item[\"fav_nums\"] = fav_nums\n",
    "        article_item[\"comment_nums\"] = comment_nums\n",
    "        article_item[\"tag\"] = tag\n",
    "        article_item['content'] = content\n",
    "\n",
    "        yield article_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline中代码的分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline主要是对spiders中爬虫的返回的数据的处理，这里可以写入到数据库，也可以写入到文件等等。\n",
    "\n",
    "下面代码中主要包括的写入到json文件以及写入到数据库，包括异步插入到数据库，还有图片的处理，这里可以定义各种需要的pipeline，当然不同的pipeline是有一定的顺序的，需要的设置是在settings配置文件中，如下，后面的数字表示的是优先级，数字越小优先级越高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobbolespiderPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "\n",
    "class JsonWithEncodingPipeline(object):\n",
    "    '''\n",
    "    返回json数据到文件\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.file = codecs.open(\"article.json\",'w',encoding=\"utf-8\")\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        lines = json.dumps(dict(item),ensure_ascii=False) + \"\\n\"\n",
    "        self.file.write(lines)\n",
    "        return item\n",
    "\n",
    "    def spider_closed(self,spider):\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class MysqlPipeline(object):\n",
    "    '''\n",
    "    插入mysql数据库\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.conn =pymysql.connect(host='192.168.1.19',port=3306,user='root',passwd='123456',db='article_spider',use_unicode=True, charset=\"utf8\")\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def process_item(self,item,spider):\n",
    "        insert_sql = '''\n",
    "        insert into jobbole_article(title,create_date,url,url_object_id,front_image_url,front_image_path,comment_nums,fav_nums,praise_nums,tag,content) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        '''\n",
    "\n",
    "        self.cursor.execute(insert_sql,(item[\"title\"],item[\"create_date\"],item[\"url\"],item[\"url_object_id\"],item[\"front_image_url\"],item[\"front_image_path\"],item[\"comment_nums\"],item[\"fav_nums\"],item[\"praise_nums\"],item[\"tag\"],item[\"content\"]))\n",
    "        self.conn.commit()\n",
    "\n",
    "\n",
    "class MysqlTwistedPipline(object):\n",
    "    '''\n",
    "    采用异步的方式插入数据\n",
    "    '''\n",
    "    def __init__(self,dbpool):\n",
    "        self.dbpool = dbpool\n",
    "\n",
    "    @classmethod\n",
    "    def from_settings(cls,settings):\n",
    "        dbparms = dict(\n",
    "            host = settings[\"MYSQL_HOST\"],\n",
    "            port = settings[\"MYSQL_PORT\"],\n",
    "            user = settings[\"MYSQL_USER\"],\n",
    "            passwd = settings[\"MYSQL_PASSWD\"],\n",
    "            db = settings[\"MYSQL_DB\"],\n",
    "            use_unicode = True,\n",
    "            charset=\"utf8\",\n",
    "        )\n",
    "        dbpool = adbapi.ConnectionPool(\"pymysql\",**dbparms)\n",
    "        return cls(dbpool)\n",
    "    \n",
    "    def process_item(self,item,spider):\n",
    "        '''\n",
    "        使用twisted将mysql插入变成异步\n",
    "        :param item:\n",
    "        :param spider:\n",
    "        :return:\n",
    "        '''\n",
    "        query = self.dbpool.runInteraction(self.do_insert,item)\n",
    "        query.addErrback(self.handle_error)\n",
    "\n",
    "    def handle_error(self,failure):\n",
    "        #处理异步插入的异常\n",
    "        print(failure)\n",
    "\n",
    "    def do_insert(self,cursor,item):\n",
    "        #具体插入数据\n",
    "        insert_sql = '''\n",
    "        insert into jobbole_article(title,create_date,url,url_object_id,front_image_url,front_image_path,comment_nums,fav_nums,praise_nums,tag,content) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "        '''\n",
    "        cursor.execute(insert_sql,(item[\"title\"],item[\"create_date\"],item[\"url\"],item[\"url_object_id\"],item[\"front_image_url\"],item[\"front_image_path\"],item[\"comment_nums\"],item[\"fav_nums\"],item[\"praise_nums\"],item[\"tag\"],item[\"content\"]))\n",
    "\n",
    "\n",
    "\n",
    "class ArticleImagePipeline(ImagesPipeline):\n",
    "    '''\n",
    "    对图片的处理\n",
    "    '''\n",
    "    def item_completed(self, results, item, info):\n",
    "\n",
    "        for ok ,value in results:\n",
    "            if ok:\n",
    "                image_file_path = value[\"path\"]\n",
    "                item['front_image_path'] = image_file_path\n",
    "            else:\n",
    "                item['front_image_path'] = \"\"\n",
    "\n",
    "\n",
    "        return item"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
