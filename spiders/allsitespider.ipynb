{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全站爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 功能介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 支持深度控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pymongo import MongoClient\n",
    "import gevent\n",
    "from gevent import monkey, pool\n",
    "from gevent.queue import Queue\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from html import unescape\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "monkey.patch_socket()\n",
    "\n",
    "# 爬虫参数\n",
    "PARAMS = {\n",
    "    # 起始url\n",
    "    'start_url': 'https://www.baidu.com',\n",
    "    #\n",
    "    'start_url_request_method': 'GET',\n",
    "    # url类型:\n",
    "    'end_type': 'PC',\n",
    "    # 并发数\n",
    "    'concurrency': 5,\n",
    "    # 爬取深度, if depth is not positive, then no depth limit\n",
    "    'depth': -1,\n",
    "    # 单个HTML的Content-Length为1M\n",
    "    'content_length': 1 * 1024 * 1024,\n",
    "    # 整站爬取总量限制, if amount is not positive, then no amount limit to urls\n",
    "    'amount': 500,\n",
    "    # 网络请求超时\n",
    "    'timeout': 5,\n",
    "    # 队列取数据超时\n",
    "    'queue_timeout': 1,\n",
    "    # 爬取延时\n",
    "    'delay': -1,\n",
    "    # 队列大小\n",
    "    'queue_size': 500,\n",
    "    # 域名允许控制\n",
    "    'allowed_domains': ['www.baidu.com'],\n",
    "    # 关键字排除\n",
    "    'exclude_keywords': [],\n",
    "    # 支持cookie\n",
    "    'cookies': {}\n",
    "}\n",
    "\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Accept-Encoding': 'gzip, deflate'\n",
    "}\n",
    "\n",
    "\n",
    "# MONGODB参数\n",
    "MONGODB = {\n",
    "    \"user\": \"xxxx\",\n",
    "    \"passwd\": \"xxxx\",\n",
    "    \"host\": \"127.0.0.1:27017\",\n",
    "    \"dbname\": \"xxxxx\"\n",
    "}\n",
    "\n",
    "\n",
    "def init_root_logger_settings(log_name='spiders', logConsole=True):\n",
    "    LOG_FORMAT = \"%(asctime)s [%(levelname)s] [%(filename)s] [%(lineno)d]: %(message)s\"\n",
    "    log_dir = os.path.join(os.path.dirname(\n",
    "        os.path.dirname(os.path.abspath(__file__))), \"logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "    fh = logging.handlers.TimedRotatingFileHandler(filename=os.path.join(log_dir, log_name),\n",
    "                                                   when='midnight', interval=1, encoding='utf-8')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.suffix = \"%Y-%m-%d.log\"\n",
    "    fh.setFormatter(formatter)\n",
    "    root_logger.addHandler(fh)\n",
    "\n",
    "    if logConsole:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        root_logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def connect_mongo(MONGODB):\n",
    "    # 少一个参数\n",
    "    client = MongoClient(\n",
    "        'mongodb://{}:{}@{}/{}'.format(MONGODB['user'],\n",
    "                                       MONGODB['passwd'],\n",
    "                                       MONGODB['host'],\n",
    "                                       MONGODB['dbname']))\n",
    "    return client[MONGODB['dbname']]\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.init_spider_params(params)\n",
    "        self.manager = UrlManager(self.params)\n",
    "        self.downloader = HtmlDownloader(self.params)\n",
    "        self.parser = HtmlParser(self.params)\n",
    "        self.data_item = DataItem(self.params)\n",
    "        self.urlQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.respQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.request_num = 0\n",
    "        self.response_num = 0\n",
    "\n",
    "    def init_spider_params(self, params):\n",
    "        self.params = PARAMS\n",
    "        if params:\n",
    "            self.params.update(params)\n",
    "\n",
    "    def is_running(self):\n",
    "        if self.parser.stop_parse:\n",
    "            if not self.urlQ.empty() or not self.respQ.empty():\n",
    "                is_running = True\n",
    "            else:\n",
    "                is_running = False\n",
    "        else:\n",
    "            is_running = True\n",
    "        return is_running\n",
    "\n",
    "    def init_task(self):\n",
    "        logging.info('Init first task.')\n",
    "        start_url = self.params.get('start_url')\n",
    "        url_item = self.manager.patch_url(start_url)\n",
    "        self.parser.all_urls.add(url_item.get('url'))\n",
    "        self.urlQ.put_nowait(url_item)\n",
    "\n",
    "    def consume_task(self):\n",
    "        try:\n",
    "            url_item = self.urlQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.request_num += 1\n",
    "            resp_list = self.downloader.get_response(url_item)\n",
    "            for response in resp_list:\n",
    "                self.respQ.put_nowait(response)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # logging.warning('Get url from queue time out.')\n",
    "\n",
    "    def produce_task(self):\n",
    "        try:\n",
    "            response = self.respQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.response_num += 1\n",
    "            url_items = self.parser.parse(response)\n",
    "            for url_item in url_items:\n",
    "                if not self.parser.stop_parse:\n",
    "                    self.urlQ.put_nowait(url_item)\n",
    "                else:\n",
    "                    break\n",
    "            self.data_item.save(response)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # logging.warning('Get response from queue time out.')\n",
    "\n",
    "    def crawl(self):\n",
    "        tp = pool.Pool(50)\n",
    "        logging.info('Spider start crawl.')\n",
    "        tp.add(gevent.spawn(self.init_task))\n",
    "        corutine_num = 1\n",
    "        while self.is_running():\n",
    "            try:\n",
    "                if self.urlQ.qsize() == self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    corutine_num += 2\n",
    "                elif self.urlQ.qsize() > self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    corutine_num += 3\n",
    "                else:\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    corutine_num += 3\n",
    "            except Exception as e:\n",
    "                logging.error('corutine error: {}'.format(e))\n",
    "        tp.join()\n",
    "        logging.info('Create {} corutines.'.format(corutine_num))\n",
    "        logging.info('Request num: {}.'.format(self.request_num))\n",
    "        logging.info('Response num: {}.'.format(self.response_num))\n",
    "        logging.info('History num: {}.'.format(self.downloader.history_num))\n",
    "        logging.info('Abstract urls: {}.'.format(\n",
    "            self.parser.all_abstract_urls))\n",
    "        logging.info('Filters urls: {}.'.format(self.parser.all_passed_urls))\n",
    "        logging.info('Error urls: {}.'.format(self.downloader.error_urls))\n",
    "        logging.info('All urls: {}.'.format(len(self.parser.all_urls)))\n",
    "        logging.info('Spider closed.')\n",
    "\n",
    "\n",
    "class UrlManager(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def normal_url(self, url, base_url):\n",
    "        new_url = unescape(url.strip())\n",
    "        if not re.match('(http|https)://', new_url):\n",
    "            new_url = urljoin(base_url, new_url)\n",
    "        return new_url[:-1] if new_url.endswith('/') else new_url\n",
    "\n",
    "    def patch_url(self, url, method='GET', data=None, parent_url_obj=None):\n",
    "        url_item = {}\n",
    "        url_item['base_url'] = parent_url_obj.get(\n",
    "            'url') if parent_url_obj else ''\n",
    "        url_item['depth'] = parent_url_obj.get(\n",
    "            'depth') + 1 if parent_url_obj else 1\n",
    "        url_item['url'] = self.normal_url(url, url_item.get('base_url'))\n",
    "        url_item['domain'] = urlparse(url_item.get('url')).netloc\n",
    "        url_item['end_type'] = self.params.get('end_type')\n",
    "        url_item['method'] = method\n",
    "        url_item['data'] = data\n",
    "\n",
    "        return url_item\n",
    "\n",
    "\n",
    "class HtmlDownloader(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.error_urls = 0\n",
    "        self.history_num = 0\n",
    "\n",
    "    def get_headers(self):\n",
    "        return HEADER\n",
    "\n",
    "    def get_proxy(self):\n",
    "        return self.params.get('proxy')\n",
    "\n",
    "    def get_cookies(self):\n",
    "        return self.params.get('cookies')\n",
    "\n",
    "    def get_timeout(self):\n",
    "        return self.params.get('timeout')\n",
    "\n",
    "    def download(self, url_item):\n",
    "        url, method, data = url_item.get('url'), url_item.get(\n",
    "            'method'), url_item.get('data')\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, headers=self.get_headers(),\n",
    "                                        proxies=self.get_proxy(), stream=True,\n",
    "                                        cookies=self.get_cookies(),\n",
    "                                        timeout=self.get_timeout())\n",
    "            else:\n",
    "                response = requests.post(url, headers=self.get_headers(),\n",
    "                                         proxies=self.get_proxy(), data=data,\n",
    "                                         stream=True,\n",
    "                                         cookies=self.get_cookies(),\n",
    "                                         timeout=self.get_timeout())\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error('Download html error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "    def get_response(self, url_item):\n",
    "        resp_list = []\n",
    "        response = self.download(url_item)\n",
    "        if response:\n",
    "            resp = {}\n",
    "            resp['url_item'] = url_item\n",
    "            resp['response'] = response\n",
    "            resp_list.append(resp)\n",
    "            for history_item in response.history:\n",
    "                resp = {}\n",
    "                resp['url_item'] = url_item\n",
    "                resp['response'] = history_item\n",
    "                resp_list.append(resp)\n",
    "                self.history_num += 1\n",
    "        return resp_list\n",
    "\n",
    "\n",
    "class HtmlParser(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.manager = UrlManager(self.params)\n",
    "        # 所有不重复url的集合,包括不符合过滤条件的url,该集合仅用来过滤重复urls,并不是实际请求的urls数目\n",
    "        self.all_urls = set()\n",
    "        # 停止解析urls\n",
    "        self.stop_parse = False\n",
    "        # 被过滤掉的urls数目\n",
    "        self.all_passed_urls = 0\n",
    "        # 提取到的urls数目\n",
    "        self.all_abstract_urls = 0\n",
    "\n",
    "    def filter_url(self, url_item):\n",
    "        url = url_item.get('url')\n",
    "        depth = url_item.get('depth')\n",
    "        amount = self.params.get('amount')\n",
    "        # 如果使用总量限制,且请求url超出总量设置,则过滤并停止提取url\n",
    "        if amount > 0 and self.all_abstract_urls > amount:\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url amount {} > {}, stop parse urls.'.format(\n",
    "                self.all_abstract_urls, self.params.get('amount')))\n",
    "            return True\n",
    "\n",
    "        # 过滤重复url\n",
    "        if url in self.all_urls:\n",
    "            return True\n",
    "        else:\n",
    "            self.all_urls.add(url)\n",
    "\n",
    "        # 支持url深度限制\n",
    "        if self.params.get('depth') > 0 and depth > self.params.get('depth'):\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url depth {} > {}, stop parse urls.'.format(\n",
    "                depth, self.params.get('depth')))\n",
    "            return True\n",
    "\n",
    "        # 支持域名过滤url\n",
    "        domain = urlparse(url).netloc\n",
    "        allowed_domains = self.params.get('allowed_domains')\n",
    "        if allowed_domains and domain not in allowed_domains:\n",
    "            return True\n",
    "\n",
    "        # 支持排除关键字过滤url\n",
    "        for keyword in self.params.get('exclude_keywords'):\n",
    "            if keyword in url:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_form_data(self, tag):\n",
    "        data = {}\n",
    "        for input in tag.find_all('input'):\n",
    "            name = input.get('name')\n",
    "            if name and input.get('type') in ['text', 'password']:\n",
    "                data[name] = input.get('value', '')\n",
    "            elif input.get('type') == 'submit':\n",
    "                name = 'submit'\n",
    "                data[name] = input.get('value', '')\n",
    "            else:\n",
    "                if data.get(name) is None:\n",
    "                    data[name] = list(input.get('value', ''))\n",
    "                else:\n",
    "                    data[name].append(input.get('value', ''))\n",
    "        return data\n",
    "\n",
    "    def abstract_urls(self, response):\n",
    "        url_items = []\n",
    "        url_item = response.get('url_item')\n",
    "        resp = response.get('response')\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        tags = soup.find_all(True)\n",
    "        for tag in tags:\n",
    "            if self.stop_parse:\n",
    "                logging.info('Stop abstract urls.')\n",
    "                break\n",
    "            method, data = 'GET', None\n",
    "            if tag.name == 'form':\n",
    "                url = tag.get('action', '')\n",
    "                method = tag.get('method')\n",
    "                data = self.parse_form_data(tag)\n",
    "            elif tag.name == 'script':\n",
    "                url = tag.get('src', '')\n",
    "            else:\n",
    "                url = tag.get('href', '')\n",
    "            sub_url_item = self.manager.patch_url(\n",
    "                url, method=method, data=data, parent_url_obj=url_item)\n",
    "            if not self.filter_url(sub_url_item):\n",
    "                url_items.append(sub_url_item)\n",
    "            else:\n",
    "                self.all_passed_urls += 1\n",
    "        return url_items\n",
    "\n",
    "    def parse(self, response):\n",
    "        url_items = []\n",
    "        if response and not self.stop_parse:\n",
    "            url_items = self.abstract_urls(response)\n",
    "        # else:\n",
    "        #     logging.warning('Response is None')\n",
    "        self.all_abstract_urls += len(url_items)\n",
    "        return url_items\n",
    "\n",
    "\n",
    "class DataItem(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        db = connect_mongo(MONGODB)\n",
    "        self.movie = db['my_crawler_urls']\n",
    "\n",
    "    def handle_url(self, url):\n",
    "        url_obj = urlparse(url)\n",
    "        port = 443 if url_obj.scheme == 'https' else 80\n",
    "        if ':80' in url_obj.netloc or ':443' in url_obj.netloc:\n",
    "            new_netloc = url_obj.netloc\n",
    "        else:\n",
    "            new_netloc = '{}:{}'.format(url_obj.netloc, port)\n",
    "\n",
    "        if url_obj.params:\n",
    "            new_params = '?{}'.format(url_obj.params)\n",
    "        else:\n",
    "            new_params = url_obj.params\n",
    "\n",
    "        if url_obj.query:\n",
    "            new_query = '?{}'.format(url_obj.query)\n",
    "        else:\n",
    "            new_query = url_obj.query\n",
    "\n",
    "        new_url = '{}://{}{}{}{}{}'.format(url_obj.scheme, new_netloc,\n",
    "                                           url_obj.path, new_params,\n",
    "                                           new_query, url_obj.fragment)\n",
    "        return new_url\n",
    "\n",
    "    def handle_title(self, data):\n",
    "        title = ''\n",
    "        response = data.get('response')\n",
    "        html = response.text\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            title_tag = soup.title\n",
    "            title = title_tag.get_text() if title_tag else ''\n",
    "        return title\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        response = data.get('response')\n",
    "        request, resp = {}, {}\n",
    "        request['headers'] = response.request.headers\n",
    "        request['url'] = response.request.url\n",
    "        request['method'] = response.request.method\n",
    "\n",
    "        resp['headers'] = response.headers\n",
    "        resp['content'] = response.content\n",
    "        resp['status_code'] = response.status_code\n",
    "        resp['url'] = self.handle_url(request['url'])\n",
    "        return request, resp\n",
    "\n",
    "    def save(self, response):\n",
    "        doc = {}\n",
    "        try:\n",
    "            request, resp = self.handle_data(response)\n",
    "            title = self.handle_title(response)\n",
    "            doc['request'] = request\n",
    "            doc['response'] = resp\n",
    "            doc['title'] = title\n",
    "            doc['site'] = self.params.get('start_url')\n",
    "            doc['end_type'] = self.params.get('end_type')\n",
    "            doc['time'] = int(time.time() * 1000)\n",
    "            self.movie.insert(doc)\n",
    "        except Exception as e:\n",
    "            logging.error('Save data to mongodb error: {}'.format(e))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_root_logger_settings()\n",
    "    spider = Spider(PARAMS)\n",
    "    spider.crawl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import logging.handlers\n",
    "from fnmatch import fnmatch\n",
    "import gevent\n",
    "from gevent import monkey, pool\n",
    "from gevent.queue import Queue\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from html import unescape\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "monkey.patch_socket()\n",
    "\n",
    "from .config import PARAMS\n",
    "from .config import get_header\n",
    "from utils.connectmongo import ConnectMongo\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        assert isinstance(params, dict)\n",
    "        self.init_spider_params(params)\n",
    "        self.manager = UrlManager(self.params)\n",
    "        self.downloader = HtmlDownloader(self.params)\n",
    "        self.parser = HtmlParser(self.params)\n",
    "        self.data_item = DataItem(self.params)\n",
    "        self.urlQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.respQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.request_num = 0\n",
    "        self.response_num = 0\n",
    "\n",
    "    def init_spider_params(self, params):\n",
    "        self.params = PARAMS\n",
    "        if params:\n",
    "            self.params.update(params)\n",
    "\n",
    "    def is_running(self):\n",
    "        if self.parser.stop_parse:\n",
    "            if not self.urlQ.empty() or not self.respQ.empty():\n",
    "                is_running = True\n",
    "            else:\n",
    "                is_running = False\n",
    "        else:\n",
    "            is_running = True\n",
    "        return is_running\n",
    "\n",
    "    def init_task(self):\n",
    "        start_url = self.params.get('start_url')\n",
    "        url_item = self.manager.patch_url(start_url)\n",
    "        self.parser.all_urls.add(url_item.get('url'))\n",
    "        self.urlQ.put_nowait(url_item)\n",
    "\n",
    "    def consume_task(self):\n",
    "        try:\n",
    "            url_item = self.urlQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.request_num += 1\n",
    "            resp_list = self.downloader.get_response(url_item)\n",
    "            for response in resp_list:\n",
    "                self.respQ.put_nowait(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    def produce_task(self):\n",
    "        try:\n",
    "            response = self.respQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.response_num += 1\n",
    "            url_items = self.parser.parse(response)\n",
    "            for url_item in url_items:\n",
    "                if not self.parser.stop_parse:\n",
    "                    self.urlQ.put_nowait(url_item)\n",
    "                else:\n",
    "                    break\n",
    "            self.data_item.save(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    def crawl(self):\n",
    "        tp = pool.Pool(50)\n",
    "        logging.info('Spider start crawl.')\n",
    "        logging.info('Init first task.')\n",
    "        tp.add(gevent.spawn(self.init_task))\n",
    "        while self.is_running():\n",
    "            try:\n",
    "                if self.urlQ.qsize() == self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                elif self.urlQ.qsize() > self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                else:\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "            except Exception as e:\n",
    "                logging.error('corutine error: {}'.format(e))\n",
    "        tp.join()\n",
    "        logging.info('Request num: {}.'.format(self.request_num))\n",
    "        logging.info('Response num: {}.'.format(self.response_num))\n",
    "        logging.info('History num: {}.'.format(self.downloader.history_num))\n",
    "        logging.info('Abstract urls: {}.'.format(self.parser.all_abstract_urls))\n",
    "        logging.info('Filters urls: {}.'.format(self.parser.all_passed_urls))\n",
    "        logging.info('Error urls: {}.'.format(self.downloader.error_urls + self.data_item.error_urls))\n",
    "        logging.info('All urls: {}.'.format(len(self.parser.all_urls)))\n",
    "        logging.info('Spider closed.')\n",
    "\n",
    "\n",
    "class UrlManager(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def normal_url(self, url, base_url):\n",
    "        new_url = unescape(url.strip())\n",
    "        if not re.match('(http|https)://', new_url):\n",
    "            new_url = urljoin(base_url, new_url)\n",
    "        return new_url[:-1] if new_url.endswith('/') else new_url\n",
    "\n",
    "    def patch_url(self, url, method='GET', data=None, parent_url_obj=None):\n",
    "        url_item = {}\n",
    "        url_item['base_url'] = parent_url_obj.get(\n",
    "            'url') if parent_url_obj else ''\n",
    "        url_item['depth'] = parent_url_obj.get(\n",
    "            'depth') + 1 if parent_url_obj else 1\n",
    "        url_item['url'] = self.normal_url(url, url_item.get('base_url'))\n",
    "        url_item['domain'] = urlparse(url_item.get('url')).netloc\n",
    "        url_item['end_type'] = self.params.get('end_type')\n",
    "        url_item['method'] = method\n",
    "        url_item['data'] = data\n",
    "\n",
    "        return url_item\n",
    "\n",
    "\n",
    "class HtmlDownloader(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.error_urls = 0\n",
    "        self.history_num = 0\n",
    "\n",
    "    def get_headers(self):\n",
    "        return get_header(self.params.get('end_type'))\n",
    "\n",
    "    def get_proxy(self):\n",
    "        return self.params.get('proxy')\n",
    "\n",
    "    def get_cookies(self):\n",
    "        return self.params.get('cookies')\n",
    "\n",
    "    def get_timeout(self):\n",
    "        return self.params.get('timeout')\n",
    "\n",
    "    def download(self, url_item):\n",
    "        url, method, data = url_item.get('url'), url_item.get(\n",
    "            'method'), url_item.get('data')\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, headers=self.get_headers(),\n",
    "                                        proxies=self.get_proxy(), stream=True,\n",
    "                                        cookies=self.get_cookies(),\n",
    "                                        timeout=self.get_timeout())\n",
    "            else:\n",
    "                response = requests.post(url, headers=self.get_headers(),\n",
    "                                         proxies=self.get_proxy(), data=data,\n",
    "                                         stream=True,\n",
    "                                         cookies=self.get_cookies(),\n",
    "                                         timeout=self.get_timeout())\n",
    "            # 过滤content-length大于1M的html下载链接\n",
    "            if int(response.headers.get('Content-Length', 0)) < self.params.get('content_length'):\n",
    "                return response\n",
    "            else:\n",
    "                logging.warning('Content length > 1M, url: {}'.format(url))\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error('Download html error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "    def get_response(self, url_item):\n",
    "        resp_list = []\n",
    "        response = self.download(url_item)\n",
    "        if response:\n",
    "            resp = {}\n",
    "            resp['url_item'] = url_item\n",
    "            resp['response'] = response\n",
    "            resp_list.append(resp)\n",
    "            for history_item in response.history:\n",
    "                resp = {}\n",
    "                resp['url_item'] = url_item\n",
    "                resp['response'] = history_item\n",
    "                resp_list.append(resp)\n",
    "                self.history_num += 1\n",
    "        return resp_list\n",
    "\n",
    "\n",
    "class HtmlParser(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.manager = UrlManager(self.params)\n",
    "        # 所有不重复url的集合,包括不符合过滤条件的url,该集合仅用来过滤重复urls,并不是实际请求的urls数目\n",
    "        self.all_urls = set()\n",
    "        # 停止解析urls\n",
    "        self.stop_parse = False\n",
    "        # 被过滤掉的urls数目\n",
    "        self.all_passed_urls = 0\n",
    "        # 提取到的urls数目\n",
    "        self.all_abstract_urls = 0\n",
    "        self.parse_url_list = []\n",
    "\n",
    "    def filter_url(self, url_item):\n",
    "        url = url_item.get('url')\n",
    "        depth = url_item.get('depth')\n",
    "        amount = self.params.get('amount')\n",
    "        # 如果使用总量限制,且请求url超出总量设置,则过滤并停止提取url\n",
    "        if amount > 0 and self.all_abstract_urls > amount:\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url amount {} > {}, stop parse urls.'.format(\n",
    "                self.all_abstract_urls, self.params.get('amount')))\n",
    "            return True\n",
    "\n",
    "        # 支持url深度限制\n",
    "        if self.params.get('depth') > 0 and depth > self.params.get('depth'):\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url depth {} > {}, stop parse urls.'.format(\n",
    "                depth, self.params.get('depth')))\n",
    "            return True\n",
    "\n",
    "        # 支持域名过滤url\n",
    "        domain = urlparse(url).netloc\n",
    "        for allowed_domain in self.params.get('allowed_domains'):\n",
    "            if not fnmatch(domain, allowed_domain):\n",
    "                return True\n",
    "\n",
    "        # 支持排除关键字过滤url\n",
    "        for keyword in self.params.get('exclude_keywords'):\n",
    "            if keyword in url:\n",
    "                return True\n",
    "\n",
    "        # 过滤重复url\n",
    "        if url in self.all_urls:\n",
    "            return True\n",
    "        else:\n",
    "            self.all_urls.add(url)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_form_data(self, tag):\n",
    "        data = {}\n",
    "        for input in tag.find_all('input'):\n",
    "            name = input.get('name')\n",
    "            if name and input.get('type') in ['text', 'password']:\n",
    "                data[name] = input.get('value', '')\n",
    "            elif input.get('type') == 'submit':\n",
    "                name = 'submit'\n",
    "                data[name] = input.get('value', '')\n",
    "            else:\n",
    "                if data.get(name) is None:\n",
    "                    data[name] = list(input.get('value', ''))\n",
    "                else:\n",
    "                    data[name].append(input.get('value', ''))\n",
    "        return data\n",
    "\n",
    "    def abstract_urls(self, response):\n",
    "        url_items = []\n",
    "        url_item = response.get('url_item')\n",
    "        resp = response.get('response')\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        tags = soup.find_all(True)\n",
    "        for tag in tags:\n",
    "            if self.stop_parse:\n",
    "                logging.info('Stop abstract urls.')\n",
    "                break\n",
    "            method, data = 'GET', None\n",
    "            if tag.name == 'form':\n",
    "                url = tag.get('action', '')\n",
    "                method = tag.get('method')\n",
    "                data = self.parse_form_data(tag)\n",
    "            elif tag.name == 'script':\n",
    "                url = tag.get('src', '')\n",
    "            else:\n",
    "                url = tag.get('href', '')\n",
    "            sub_url_item = self.manager.patch_url(\n",
    "                url, method=method, data=data, parent_url_obj=url_item)\n",
    "            if not self.filter_url(sub_url_item):\n",
    "                url_items.append(sub_url_item)\n",
    "                self.parse_url_list.append(sub_url_item.get('url'))\n",
    "            else:\n",
    "                self.all_passed_urls += 1\n",
    "        return url_items\n",
    "\n",
    "    def parse(self, response):\n",
    "        url_items = []\n",
    "        if response and not self.stop_parse:\n",
    "            url_items = self.abstract_urls(response)\n",
    "        self.all_abstract_urls += len(url_items)\n",
    "        return url_items\n",
    "\n",
    "\n",
    "class DataItem(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        db = ConnectMongo().db\n",
    "        self.movie = db['crawler_urls']\n",
    "        self.error_urls = 0\n",
    "        # 入库前再清洗一遍相同的url\n",
    "        self.res_urls = set()\n",
    "        self.duplicate_urls = 0\n",
    "\n",
    "    def handle_url(self, url):\n",
    "        url_obj = urlparse(url)\n",
    "        port = 443 if url_obj.scheme == 'https' else 80\n",
    "        if ':80' in url_obj.netloc or ':443' in url_obj.netloc:\n",
    "            new_netloc = url_obj.netloc\n",
    "        else:\n",
    "            new_netloc = '{}:{}'.format(url_obj.netloc, port)\n",
    "\n",
    "        if url_obj.params:\n",
    "            new_params = '?{}'.format(url_obj.params)\n",
    "        else:\n",
    "            new_params = url_obj.params\n",
    "\n",
    "        if url_obj.query:\n",
    "            new_query = '?{}'.format(url_obj.query)\n",
    "        else:\n",
    "            new_query = url_obj.query\n",
    "\n",
    "        new_url = '{}://{}{}{}{}{}'.format(url_obj.scheme, new_netloc,\n",
    "                                           url_obj.path, new_params,\n",
    "                                           new_query, url_obj.fragment)\n",
    "        return new_url\n",
    "\n",
    "    def handle_title(self, data):\n",
    "        title = ''\n",
    "        response = data.get('response')\n",
    "        html = response.content\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            title_tag = soup.title\n",
    "            title = title_tag.get_text() if title_tag else ''\n",
    "        return title\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        response = data.get('response')\n",
    "        request, resp = {}, {}\n",
    "        request['headers'] = response.request.headers\n",
    "        request['url'] = response.request.url\n",
    "        request['method'] = response.request.method\n",
    "\n",
    "        resp['headers'] = response.headers\n",
    "        resp['content'] = response.content\n",
    "        resp['status_code'] = response.status_code\n",
    "        url = self.handle_url(request['url'])\n",
    "        resp['url'] = url\n",
    "        if url not in self.res_urls:\n",
    "            self.res_urls.add(url)\n",
    "            return request, resp\n",
    "        else:\n",
    "            self.duplicate_urls += 1\n",
    "            return None\n",
    "\n",
    "\n",
    "    def save(self, response):\n",
    "        doc = {}\n",
    "        url_item = response.get('url_item')\n",
    "        try:\n",
    "            data = self.handle_data(response)\n",
    "            if data:\n",
    "                request, resp = data[0], data[1]\n",
    "                title = self.handle_title(response)\n",
    "                doc['request'] = request\n",
    "                doc['response'] = resp\n",
    "                doc['title'] = title\n",
    "                doc['site'] = self.params.get('start_url')\n",
    "                doc['end_type'] = self.params.get('end_type')\n",
    "                doc['time'] = int(time.time() * 1000)\n",
    "                self.movie.insert(doc)\n",
    "        except Exception as e:\n",
    "            logging.error('Save data to mongodb error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     for i in range(5):\n",
    "#         spider = Spider(PARAMS)\n",
    "#         spider.crawl()\n",
    "#         time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
