{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全站爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬虫引擎介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 爬取指定站点所有链接及html,支持过滤\n",
    "* 使用gevent和requests来实现协程异步高并发\n",
    "* 使用生产者-消费者模式\n",
    "* 使用mongodb作数据存储\n",
    "* 性能1分钟500+请求\n",
    "* 配置灵活"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 功能介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 支持总量控制\n",
    "2. 支持深度控制\n",
    "3. 支持cookies\n",
    "4. 支持代理proxy\n",
    "5. 支持GET和POST方式请求\n",
    "6. 支持允许域名限制,支持域名通配符匹配\n",
    "7. 支持关键字排除特定url\n",
    "8. 支持HTML的content-length过滤\n",
    "9. 自动过滤重复url\n",
    "10. 支持history爬取解析和过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置参数介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置参数是一个dict, 默认参数如下(可更改):\n",
    "```python\n",
    "# 爬虫参数\n",
    "PARAMS = {\n",
    "    # 起始url\n",
    "    'start_url': 'https://www.baidu.com',\n",
    "    'start_url_request_method': 'GET',\n",
    "    # url类型:\n",
    "    'end_type': 'PC',\n",
    "    # 爬取深度, if depth is not positive, then no depth limit\n",
    "    'depth': -1,\n",
    "    # 单个HTML的Content-Length为1M\n",
    "    'content_length': 1 * 1024 * 1024,\n",
    "    # 整站爬取总量限制, if amount is not positive, then no amount limit to urls\n",
    "    'amount': 100,\n",
    "    # 网络请求超时\n",
    "    'timeout': 5,\n",
    "    # 队列取数据超时\n",
    "    'queue_timeout': 1,\n",
    "    # 爬取延时\n",
    "    'delay': -1,  # 暂不支持\n",
    "    # 队列大小\n",
    "    'queue_size': 500,\n",
    "    # 域名允许控制\n",
    "    'allowed_domains': ['www.baidu.com'],\n",
    "    # 关键字排除\n",
    "    'exclude_keywords': [],\n",
    "    # 支持cookie\n",
    "    'cookies': {}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.allsitespider import AllSiteSpider\n",
    "\n",
    "def crawl_all(params=None):\n",
    "    allsiteSpider = Spider(params)\n",
    "    allsiteSpider.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pymongo import MongoClient\n",
    "import gevent\n",
    "from gevent import monkey, pool\n",
    "from gevent.queue import Queue\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from html import unescape\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "monkey.patch_socket()\n",
    "\n",
    "# 爬虫参数\n",
    "PARAMS = {\n",
    "    # 起始url\n",
    "    'start_url': 'https://www.baidu.com',\n",
    "    #\n",
    "    'start_url_request_method': 'GET',\n",
    "    # url类型:\n",
    "    'end_type': 'PC',\n",
    "    # 并发数\n",
    "    'concurrency': 5,\n",
    "    # 爬取深度, if depth is not positive, then no depth limit\n",
    "    'depth': -1,\n",
    "    # 单个HTML的Content-Length为1M\n",
    "    'content_length': 1 * 1024 * 1024,\n",
    "    # 整站爬取总量限制, if amount is not positive, then no amount limit to urls\n",
    "    'amount': 500,\n",
    "    # 网络请求超时\n",
    "    'timeout': 5,\n",
    "    # 队列取数据超时\n",
    "    'queue_timeout': 1,\n",
    "    # 爬取延时\n",
    "    'delay': -1,\n",
    "    # 队列大小\n",
    "    'queue_size': 500,\n",
    "    # 域名允许控制\n",
    "    'allowed_domains': ['www.baidu.com'],\n",
    "    # 关键字排除\n",
    "    'exclude_keywords': [],\n",
    "    # 支持cookie\n",
    "    'cookies': {}\n",
    "}\n",
    "\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Accept-Encoding': 'gzip, deflate'\n",
    "}\n",
    "\n",
    "\n",
    "# MONGODB参数\n",
    "MONGODB = {\n",
    "    \"user\": \"xxxx\",\n",
    "    \"passwd\": \"xxxx\",\n",
    "    \"host\": \"127.0.0.1:27017\",\n",
    "    \"dbname\": \"xxxxx\"\n",
    "}\n",
    "\n",
    "\n",
    "def init_root_logger_settings(log_name='spiders', logConsole=True):\n",
    "    LOG_FORMAT = \"%(asctime)s [%(levelname)s] [%(filename)s] [%(lineno)d]: %(message)s\"\n",
    "    log_dir = os.path.join(os.path.dirname(\n",
    "        os.path.dirname(os.path.abspath(__file__))), \"logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "    fh = logging.handlers.TimedRotatingFileHandler(filename=os.path.join(log_dir, log_name),\n",
    "                                                   when='midnight', interval=1, encoding='utf-8')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.suffix = \"%Y-%m-%d.log\"\n",
    "    fh.setFormatter(formatter)\n",
    "    root_logger.addHandler(fh)\n",
    "\n",
    "    if logConsole:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        root_logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def connect_mongo(MONGODB):\n",
    "    client = MongoClient(\n",
    "        'mongodb://{}:{}@{}/{}'.format(MONGODB['user'],\n",
    "                                       MONGODB['passwd'],\n",
    "                                       MONGODB['host'],\n",
    "                                       MONGODB['dbname']))\n",
    "    return client[MONGODB['dbname']]\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        assert isinstance(params, dict)\n",
    "        self.init_spider_params(params)\n",
    "        self.manager = UrlManager(self.params)\n",
    "        self.downloader = HtmlDownloader(self.params)\n",
    "        self.parser = HtmlParser(self.params)\n",
    "        self.data_item = DataItem(self.params)\n",
    "        self.urlQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.respQ = Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.request_num = 0\n",
    "        self.response_num = 0\n",
    "\n",
    "    def init_spider_params(self, params):\n",
    "        self.params = PARAMS\n",
    "        if params:\n",
    "            self.params.update(params)\n",
    "\n",
    "    def is_running(self):\n",
    "        if self.parser.stop_parse:\n",
    "            if not self.urlQ.empty() or not self.respQ.empty():\n",
    "                is_running = True\n",
    "            else:\n",
    "                is_running = False\n",
    "        else:\n",
    "            is_running = True\n",
    "        return is_running\n",
    "\n",
    "    def init_task(self):\n",
    "        start_url = self.params.get('start_url')\n",
    "        url_item = self.manager.patch_url(start_url)\n",
    "        self.parser.all_urls.add(url_item.get('url'))\n",
    "        self.urlQ.put_nowait(url_item)\n",
    "\n",
    "    def consume_task(self):\n",
    "        try:\n",
    "            url_item = self.urlQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.request_num += 1\n",
    "            resp_list = self.downloader.get_response(url_item)\n",
    "            for response in resp_list:\n",
    "                self.respQ.put_nowait(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    def produce_task(self):\n",
    "        try:\n",
    "            response = self.respQ.get(timeout=self.params.get('queue_timeout'))\n",
    "            self.response_num += 1\n",
    "            url_items = self.parser.parse(response)\n",
    "            for url_item in url_items:\n",
    "                if not self.parser.stop_parse:\n",
    "                    self.urlQ.put_nowait(url_item)\n",
    "                else:\n",
    "                    break\n",
    "            self.data_item.save(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    def crawl(self):\n",
    "        tp = pool.Pool(50)\n",
    "        logging.info('Spider start crawl.')\n",
    "        logging.info('Init first task.')\n",
    "        tp.add(gevent.spawn(self.init_task))\n",
    "        while self.is_running():\n",
    "            try:\n",
    "                if self.urlQ.qsize() == self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                elif self.urlQ.qsize() > self.respQ.qsize():\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                else:\n",
    "                    tp.add(gevent.spawn(self.consume_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "                    tp.add(gevent.spawn(self.produce_task))\n",
    "            except Exception as e:\n",
    "                logging.error('corutine error: {}'.format(e))\n",
    "        tp.join()\n",
    "        logging.info('Request num: {}.'.format(self.request_num))\n",
    "        logging.info('Response num: {}.'.format(self.response_num))\n",
    "        logging.info('History num: {}.'.format(self.downloader.history_num))\n",
    "        logging.info('Abstract urls: {}.'.format(self.parser.all_abstract_urls))\n",
    "        logging.info('Filters urls: {}.'.format(self.parser.all_passed_urls))\n",
    "        logging.info('Error urls: {}.'.format(self.downloader.error_urls + self.data_item.error_urls))\n",
    "        logging.info('All urls: {}.'.format(len(self.parser.all_urls)))\n",
    "        logging.info('Spider closed.')\n",
    "\n",
    "\n",
    "class UrlManager(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def normal_url(self, url, base_url):\n",
    "        new_url = unescape(url.strip())\n",
    "        if not re.match('(http|https)://', new_url):\n",
    "            new_url = urljoin(base_url, new_url)\n",
    "        return new_url[:-1] if new_url.endswith('/') else new_url\n",
    "\n",
    "    def patch_url(self, url, method='GET', data=None, parent_url_obj=None):\n",
    "        url_item = {}\n",
    "        url_item['base_url'] = parent_url_obj.get(\n",
    "            'url') if parent_url_obj else ''\n",
    "        url_item['depth'] = parent_url_obj.get(\n",
    "            'depth') + 1 if parent_url_obj else 1\n",
    "        url_item['url'] = self.normal_url(url, url_item.get('base_url'))\n",
    "        url_item['domain'] = urlparse(url_item.get('url')).netloc\n",
    "        url_item['end_type'] = self.params.get('end_type')\n",
    "        url_item['method'] = method\n",
    "        url_item['data'] = data\n",
    "\n",
    "        return url_item\n",
    "\n",
    "\n",
    "class HtmlDownloader(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.error_urls = 0\n",
    "        self.history_num = 0\n",
    "\n",
    "    def get_headers(self):\n",
    "        return get_header(self.params.get('end_type'))\n",
    "\n",
    "    def get_proxy(self):\n",
    "        return self.params.get('proxy')\n",
    "\n",
    "    def get_cookies(self):\n",
    "        return self.params.get('cookies')\n",
    "\n",
    "    def get_timeout(self):\n",
    "        return self.params.get('timeout')\n",
    "\n",
    "    def download(self, url_item):\n",
    "        url, method, data = url_item.get('url'), url_item.get(\n",
    "            'method'), url_item.get('data')\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, headers=self.get_headers(),\n",
    "                                        proxies=self.get_proxy(), stream=True,\n",
    "                                        cookies=self.get_cookies(),\n",
    "                                        timeout=self.get_timeout())\n",
    "            else:\n",
    "                response = requests.post(url, headers=self.get_headers(),\n",
    "                                         proxies=self.get_proxy(), data=data,\n",
    "                                         stream=True,\n",
    "                                         cookies=self.get_cookies(),\n",
    "                                         timeout=self.get_timeout())\n",
    "            # 过滤content-length大于1M的html下载链接\n",
    "            if int(response.headers.get('Content-Length', 0)) < self.params.get('content_length'):\n",
    "                return response\n",
    "            else:\n",
    "                logging.warning('Content length > 1M, url: {}'.format(url))\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error('Download html error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "    def get_response(self, url_item):\n",
    "        resp_list = []\n",
    "        response = self.download(url_item)\n",
    "        if response:\n",
    "            resp = {}\n",
    "            resp['url_item'] = url_item\n",
    "            resp['response'] = response\n",
    "            resp_list.append(resp)\n",
    "            for history_item in response.history:\n",
    "                resp = {}\n",
    "                resp['url_item'] = url_item\n",
    "                resp['response'] = history_item\n",
    "                resp_list.append(resp)\n",
    "                self.history_num += 1\n",
    "        return resp_list\n",
    "\n",
    "\n",
    "class HtmlParser(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.manager = UrlManager(self.params)\n",
    "        # 所有不重复url的集合,包括不符合过滤条件的url,该集合仅用来过滤重复urls,并不是实际请求的urls数目\n",
    "        self.all_urls = set()\n",
    "        # 停止解析urls\n",
    "        self.stop_parse = False\n",
    "        # 被过滤掉的urls数目\n",
    "        self.all_passed_urls = 0\n",
    "        # 提取到的urls数目\n",
    "        self.all_abstract_urls = 0\n",
    "        self.parse_url_list = []\n",
    "\n",
    "    def filter_url(self, url_item):\n",
    "        url = url_item.get('url')\n",
    "        depth = url_item.get('depth')\n",
    "        amount = self.params.get('amount')\n",
    "        # 如果使用总量限制,且请求url超出总量设置,则过滤并停止提取url\n",
    "        if amount > 0 and self.all_abstract_urls > amount:\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url amount {} > {}, stop parse urls.'.format(\n",
    "                self.all_abstract_urls, self.params.get('amount')))\n",
    "            return True\n",
    "\n",
    "        # 支持url深度限制\n",
    "        if self.params.get('depth') > 0 and depth > self.params.get('depth'):\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url depth {} > {}, stop parse urls.'.format(\n",
    "                depth, self.params.get('depth')))\n",
    "            return True\n",
    "\n",
    "        # 支持域名过滤url\n",
    "        domain = urlparse(url).netloc\n",
    "        for allowed_domain in self.params.get('allowed_domains'):\n",
    "            if not fnmatch(domain, allowed_domain):\n",
    "                return True\n",
    "\n",
    "        # 支持排除关键字过滤url\n",
    "        for keyword in self.params.get('exclude_keywords'):\n",
    "            if keyword in url:\n",
    "                return True\n",
    "\n",
    "        # 过滤重复url\n",
    "        if url in self.all_urls:\n",
    "            return True\n",
    "        else:\n",
    "            self.all_urls.add(url)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_form_data(self, tag):\n",
    "        data = {}\n",
    "        for input in tag.find_all('input'):\n",
    "            name = input.get('name')\n",
    "            if name and input.get('type') in ['text', 'password']:\n",
    "                data[name] = input.get('value', '')\n",
    "            elif input.get('type') == 'submit':\n",
    "                name = 'submit'\n",
    "                data[name] = input.get('value', '')\n",
    "            else:\n",
    "                if data.get(name) is None:\n",
    "                    data[name] = list(input.get('value', ''))\n",
    "                else:\n",
    "                    data[name].append(input.get('value', ''))\n",
    "        return data\n",
    "\n",
    "    def abstract_urls(self, response):\n",
    "        url_items = []\n",
    "        url_item = response.get('url_item')\n",
    "        resp = response.get('response')\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        tags = soup.find_all(True)\n",
    "        for tag in tags:\n",
    "            if self.stop_parse:\n",
    "                logging.info('Stop abstract urls.')\n",
    "                break\n",
    "            method, data = 'GET', None\n",
    "            if tag.name == 'form':\n",
    "                url = tag.get('action', '')\n",
    "                method = tag.get('method')\n",
    "                data = self.parse_form_data(tag)\n",
    "            elif tag.name == 'script':\n",
    "                url = tag.get('src', '')\n",
    "            else:\n",
    "                url = tag.get('href', '')\n",
    "            sub_url_item = self.manager.patch_url(\n",
    "                url, method=method, data=data, parent_url_obj=url_item)\n",
    "            if not self.filter_url(sub_url_item):\n",
    "                url_items.append(sub_url_item)\n",
    "                self.parse_url_list.append(sub_url_item.get('url'))\n",
    "            else:\n",
    "                self.all_passed_urls += 1\n",
    "        return url_items\n",
    "\n",
    "    def parse(self, response):\n",
    "        url_items = []\n",
    "        if response and not self.stop_parse:\n",
    "            url_items = self.abstract_urls(response)\n",
    "        self.all_abstract_urls += len(url_items)\n",
    "        return url_items\n",
    "\n",
    "\n",
    "class DataItem(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        db = ConnectMongo().db\n",
    "        self.movie = db['crawler_urls']\n",
    "        self.error_urls = 0\n",
    "        # 入库前再清洗一遍相同的url\n",
    "        self.res_urls = set()\n",
    "        self.duplicate_urls = 0\n",
    "\n",
    "    def handle_url(self, url):\n",
    "        url_obj = urlparse(url)\n",
    "        port = 443 if url_obj.scheme == 'https' else 80\n",
    "        if ':80' in url_obj.netloc or ':443' in url_obj.netloc:\n",
    "            new_netloc = url_obj.netloc\n",
    "        else:\n",
    "            new_netloc = '{}:{}'.format(url_obj.netloc, port)\n",
    "\n",
    "        if url_obj.params:\n",
    "            new_params = '?{}'.format(url_obj.params)\n",
    "        else:\n",
    "            new_params = url_obj.params\n",
    "\n",
    "        if url_obj.query:\n",
    "            new_query = '?{}'.format(url_obj.query)\n",
    "        else:\n",
    "            new_query = url_obj.query\n",
    "\n",
    "        new_url = '{}://{}{}{}{}{}'.format(url_obj.scheme, new_netloc,\n",
    "                                           url_obj.path, new_params,\n",
    "                                           new_query, url_obj.fragment)\n",
    "        return new_url\n",
    "\n",
    "    def handle_title(self, data):\n",
    "        title = ''\n",
    "        response = data.get('response')\n",
    "        html = response.content\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            title_tag = soup.title\n",
    "            title = title_tag.get_text() if title_tag else ''\n",
    "        return title\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        response = data.get('response')\n",
    "        request, resp = {}, {}\n",
    "        request['headers'] = response.request.headers\n",
    "        request['url'] = response.request.url\n",
    "        request['method'] = response.request.method\n",
    "\n",
    "        resp['headers'] = response.headers\n",
    "        resp['content'] = response.content\n",
    "        resp['status_code'] = response.status_code\n",
    "        url = self.handle_url(request['url'])\n",
    "        resp['url'] = url\n",
    "        if url not in self.res_urls:\n",
    "            self.res_urls.add(url)\n",
    "            return request, resp\n",
    "        else:\n",
    "            self.duplicate_urls += 1\n",
    "            return None\n",
    "\n",
    "\n",
    "    def save(self, response):\n",
    "        doc = {}\n",
    "        url_item = response.get('url_item')\n",
    "        try:\n",
    "            data = self.handle_data(response)\n",
    "            if data:\n",
    "                request, resp = data[0], data[1]\n",
    "                title = self.handle_title(response)\n",
    "                doc['request'] = request\n",
    "                doc['response'] = resp\n",
    "                doc['title'] = title\n",
    "                doc['site'] = self.params.get('start_url')\n",
    "                doc['end_type'] = self.params.get('end_type')\n",
    "                doc['time'] = int(time.time() * 1000)\n",
    "                self.movie.insert(doc)\n",
    "        except Exception as e:\n",
    "            logging.error('Save data to mongodb error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spider = Spider(PARAMS)\n",
    "    spider.crawl()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/07/2018 15:55:21 [INFO] [allspider.py] [173]: Spider start crawl.\n",
    "05/07/2018 15:55:21 [INFO] [allspider.py] [174]: Init first task.\n",
    "05/07/2018 15:55:24 [INFO] [allspider.py] [319]: Current url amount 546 > 500, stop parse urls.\n",
    "05/07/2018 15:55:24 [INFO] [allspider.py] [372]: Stop abstract urls.\n",
    "05/07/2018 15:55:30 [ERROR] [allspider.py] [276]: Download html error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))\n",
    "05/07/2018 15:55:30 [ERROR] [allspider.py] [277]: Error url info: {'base_url': 'http://www.baidu.com/more', 'depth': 3, 'url': 'http://www.baidu.com/s', 'domain': 'www.baidu.com', 'end_type': 'PC', 'method': None, 'data': {'bs': ['l', 'v'], 'f': ['8'], 'rsv_bp': ['1'], 'rsv_spt': ['3'], 'wd': [], 'submit': '百度一下'}}\n",
    "05/07/2018 15:55:45 [ERROR] [allspider.py] [276]: Download html error: HTTPSConnectionPool(host='gupiao.baidu.com', port=443): Read timed out. (read timeout=5)\n",
    "05/07/2018 15:55:45 [ERROR] [allspider.py] [277]: Error url info: {'base_url': 'https://www.baidu.com/s?wd=%E5%87%A4%E5%87%B0%E7%BD%91&tn=SE_PclogoS_8whnvm25&usm=2&ie=utf-8&rsv_cq=%E4%BB%8A%E6%97%A5%E6%96%B0%E9%B2%9C%E4%BA%8B&rsv_dl=0_right_recommends_merge_20826&euri=9c5765965dd84ec5980f63887e9e4881', 'depth': 4, 'url': 'http://www.baidu.com/link?url=ooX2ACdw5niMsKgSaUSqoIq0lt_NrSLXFM9Wpfq7WbHCBBydeIn69l6JObBWGlWuCw77KNsB7cYHm3Li8VlShLMwlxoidSK-uhRu1h8uOYy', 'domain': 'www.baidu.com', 'end_type': 'PC', 'method': 'GET', 'data': None}\n",
    "05/07/2018 15:56:26 [ERROR] [allspider.py] [467]: Save data to mongodb error: HTTPConnectionPool(host='image.baidu.com', port=80): Read timed out.\n",
    "05/07/2018 15:56:26 [ERROR] [allspider.py] [468]: Error url info: {'base_url': 'https://www.baidu.com/s?wd=%E7%99%BE%E5%BA%A6%E5%A5%BD%E7%9C%8B&tn=SE_PclogoS_8whnvm25&usm=2&ie=utf-8&rsv_cq=%E4%BB%8A%E6%97%A5%E6%96%B0%E9%B2%9C%E4%BA%8B&rsv_dl=0_right_recommends_merge_20826&euri=6b0794120dec11e68e38008cfaeb7e18', 'depth': 4, 'url': 'http://www.baidu.com/link?url=nwwUQ8I1ZkG9-pfL9CPKMUoz1pJrfZkS8zhtlqZ3Xn9akBV5XmISb_KOe-XBD3wy', 'domain': 'www.baidu.com', 'end_type': 'PC', 'method': 'GET', 'data': None}\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [196]: Create 1404 corutines.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [197]: Request num: 544.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [198]: Response num: 767.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [199]: History num: 225.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [201]: Abstract urls: 546.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [202]: Filters urls: 12423.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [203]: Error urls: 3.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [204]: All urls: 547.\n",
    "05/07/2018 15:56:31 [INFO] [allspider.py] [205]: Spider closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全站爬虫使用asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import logging.handlers\n",
    "import asyncio\n",
    "import requests\n",
    "from html import unescape\n",
    "from fnmatch import fnmatch\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import traceback\n",
    "from pprint import pprint\n",
    "\n",
    "# 爬虫参数\n",
    "PARAMS = {\n",
    "    # 起始url\n",
    "    'start_url': 'https://www.baidu.com',\n",
    "    # 'start_url': 'http://2code.top/gbk.php',\n",
    "    #\n",
    "    'start_url_request_method': 'GET',\n",
    "    # url类型:\n",
    "    'end_type': 'PC',\n",
    "    # 爬取深度, if depth is not positive, then no depth limit\n",
    "    'depth': -1,\n",
    "    # 单个HTML的Content-Length为1M\n",
    "    'content_length': 1 * 1024 * 1024,\n",
    "    # 整站爬取总量限制, if amount is not positive, then no amount limit to urls\n",
    "    'amount': 500,\n",
    "    # 网络请求超时\n",
    "    'timeout': 5,\n",
    "    # 队列取数据超时\n",
    "    'queue_timeout': 1,\n",
    "    # 爬取延时\n",
    "    'delay': -1,\n",
    "    # 队列大小\n",
    "    'queue_size': 500,\n",
    "    # 域名允许控制\n",
    "    'allowed_domains': ['www.baidu.com'],\n",
    "    # 'allowed_domains': [],\n",
    "    # 关键字排除\n",
    "    'exclude_keywords': [],\n",
    "    # 支持cookie\n",
    "    'cookies': {}\n",
    "}\n",
    "\n",
    "HEADER = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Accept-Encoding': 'gzip, deflate'\n",
    "}\n",
    "\n",
    "# MONGODB参数\n",
    "MONGODB = {\n",
    "    \"user\": \"xxx\",\n",
    "    \"passwd\": \"xxxx\",\n",
    "    \"host\": \"127.0.0.1:27017\",\n",
    "    \"dbname\": \"xxxx\"\n",
    "}\n",
    "\n",
    "\n",
    "def init_root_logger_settings(log_name='spiders', logConsole=True):\n",
    "    LOG_FORMAT = \"%(asctime)s [%(levelname)s] [%(filename)s] [%(lineno)d]: %(message)s\"\n",
    "    log_dir = os.path.join(os.path.dirname(\n",
    "        os.path.dirname(os.path.abspath(__file__))), \"logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "    fh = logging.handlers.TimedRotatingFileHandler(filename=os.path.join(log_dir, log_name),\n",
    "                                                   when='midnight', interval=1, encoding='utf-8')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.suffix = \"%Y-%m-%d.log\"\n",
    "    fh.setFormatter(formatter)\n",
    "    root_logger.addHandler(fh)\n",
    "\n",
    "    if logConsole:\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.DEBUG)\n",
    "        ch.setFormatter(formatter)\n",
    "        root_logger.addHandler(ch)\n",
    "\n",
    "\n",
    "def connect_mongo(MONGODB):\n",
    "    client = MongoClient(\n",
    "        'mongodb://{}:{}@{}/{}'.format(MONGODB['user'],\n",
    "                                       MONGODB['passwd'],\n",
    "                                       MONGODB['host'],\n",
    "                                       MONGODB['dbname']))\n",
    "    return client[MONGODB['dbname']]\n",
    "\n",
    "\n",
    "class Spider(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        assert isinstance(params, dict)\n",
    "        self.init_spider_params(params)\n",
    "        self.manager = UrlManager(self.params)\n",
    "        self.downloader = HtmlDownloader(self.params)\n",
    "        self.parser = HtmlParser(self.params)\n",
    "        self.data_item = DataItem(self.params)\n",
    "        self.urlQ = asyncio.Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.respQ = asyncio.Queue(maxsize=self.params.get('queue_size'))\n",
    "        self.request_num = 0\n",
    "        self.response_num = 0\n",
    "\n",
    "    def init_spider_params(self, params):\n",
    "        self.params = PARAMS\n",
    "        if params:\n",
    "            self.params.update(params)\n",
    "\n",
    "    def is_running(self):\n",
    "        if self.parser.stop_parse:\n",
    "            if not self.urlQ.empty() or not self.respQ.empty():\n",
    "                is_running = True\n",
    "            else:\n",
    "                is_running = False\n",
    "        else:\n",
    "            is_running = True\n",
    "        return is_running\n",
    "\n",
    "    async def init_task(self):\n",
    "        start_url = self.params.get('start_url')\n",
    "        url_item = self.manager.patch_url(start_url)\n",
    "        self.parser.all_urls.add(url_item.get('url'))\n",
    "        self.urlQ.put_nowait(url_item)\n",
    "\n",
    "    async def consume_task(self):\n",
    "        try:\n",
    "            url_item = await asyncio.wait_for(self.urlQ.get(), self.params.get('queue_timeout'))\n",
    "            self.request_num += 1\n",
    "            resp_list = await self.downloader.get_response(url_item)\n",
    "            for response in resp_list:\n",
    "                self.respQ.put_nowait(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    async def produce_task(self):\n",
    "        try:\n",
    "            response = await asyncio.wait_for(self.respQ.get(), self.params.get('queue_timeout'))\n",
    "            self.response_num += 1\n",
    "            url_items = await self.parser.parse(response)\n",
    "            for url_item in url_items:\n",
    "                if not self.parser.stop_parse:\n",
    "                    self.urlQ.put_nowait(url_item)\n",
    "                else:\n",
    "                    break\n",
    "            await self.data_item.save(response)\n",
    "        except Exception as e:\n",
    "            if self.urlQ.empty() and self.respQ.empty():\n",
    "                self.parser.stop_parse = True\n",
    "\n",
    "    async def tasks(self, loop):\n",
    "        logging.info('Start to create consume and produce tasks.')\n",
    "        self.task_num = 0\n",
    "        while self.is_running():\n",
    "            task_list = []\n",
    "            try:\n",
    "                if self.urlQ.qsize() == self.respQ.qsize():\n",
    "                    consumer = loop.create_task(self.consume_task())\n",
    "                    producer = loop.create_task(self.produce_task())\n",
    "                    task_list.append(consumer)\n",
    "                    task_list.append(producer)\n",
    "                    self.task_num += 2\n",
    "                elif self.urlQ.qsize() > self.respQ.qsize():\n",
    "                    consumer1 = loop.create_task(self.consume_task())\n",
    "                    consumer2 = loop.create_task(self.consume_task())\n",
    "                    producer = loop.create_task(self.produce_task())\n",
    "                    task_list.append(consumer1)\n",
    "                    task_list.append(consumer2)\n",
    "                    task_list.append(producer)\n",
    "                    self.task_num += 3\n",
    "                else:\n",
    "                    consumer = loop.create_task(self.consume_task())\n",
    "                    producer1 = loop.create_task(self.produce_task())\n",
    "                    producer2 = loop.create_task(self.produce_task())\n",
    "                    task_list.append(consumer)\n",
    "                    task_list.append(producer1)\n",
    "                    task_list.append(producer2)\n",
    "                    self.task_num += 3\n",
    "            except Exception as e:\n",
    "                logging.error('corutine error: {}'.format(e))\n",
    "\n",
    "            await asyncio.gather(*task_list, return_exceptions=True)\n",
    "        # await asyncio.wait(task_list)\n",
    "\n",
    "    def crawl(self):\n",
    "        logging.info('Spider started!')\n",
    "        start_time = datetime.now()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        try:\n",
    "            loop.run_until_complete(self.init_task())\n",
    "            loop.run_until_complete(self.tasks(loop))\n",
    "        except KeyboardInterrupt:\n",
    "            for task in asyncio.Task.all_tasks():\n",
    "                task.cancel()\n",
    "            loop.stop()\n",
    "            loop.run_forever()\n",
    "        finally:\n",
    "            end_time = datetime.now()\n",
    "            logging.info('Request num: {}.'.format(self.request_num))\n",
    "            logging.info('Response num: {}.'.format(self.response_num))\n",
    "            logging.info('History num: {}.'.format(self.downloader.history_num))\n",
    "            logging.info('Abstract urls: {}.'.format(self.parser.all_abstract_urls))\n",
    "            logging.info('Filters urls: {}.'.format(self.parser.all_passed_urls))\n",
    "            logging.info('Error urls: {}.'.format(self.downloader.error_urls + self.data_item.error_urls))\n",
    "            logging.info('All urls: {}.'.format(len(self.parser.all_urls)))\n",
    "            logging.info('Time usage: {}'.format(end_time - start_time))\n",
    "            logging.info('Spider finished!')\n",
    "            logging.info('Create {} tasks'.format(self.task_num))\n",
    "            loop.close()\n",
    "\n",
    "\n",
    "class UrlManager(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def normal_url(self, url, base_url):\n",
    "        new_url = unescape(url.strip())\n",
    "        if not re.match('(http|https)://', new_url):\n",
    "            new_url = urljoin(base_url, new_url)\n",
    "        return new_url[:-1] if new_url.endswith('/') else new_url\n",
    "\n",
    "    def patch_url(self, url, method='GET', data=None, parent_url_obj=None):\n",
    "        url_item = {}\n",
    "        url_item['base_url'] = parent_url_obj.get(\n",
    "            'url') if parent_url_obj else ''\n",
    "        url_item['depth'] = parent_url_obj.get(\n",
    "            'depth') + 1 if parent_url_obj else 1\n",
    "        url_item['url'] = self.normal_url(url, url_item.get('base_url'))\n",
    "        url_item['domain'] = urlparse(url_item.get('url')).netloc\n",
    "        url_item['end_type'] = self.params.get('end_type')\n",
    "        url_item['method'] = method\n",
    "        url_item['data'] = data\n",
    "\n",
    "        return url_item\n",
    "\n",
    "\n",
    "class HtmlDownloader(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.error_urls = 0\n",
    "        self.history_num = 0\n",
    "\n",
    "    def get_headers(self):\n",
    "        return HEADER\n",
    "        # return get_header(self.params.get('end_type'))\n",
    "\n",
    "    def get_proxy(self):\n",
    "        return self.params.get('proxy')\n",
    "\n",
    "    def get_cookies(self):\n",
    "        return self.params.get('cookies')\n",
    "\n",
    "    def get_timeout(self):\n",
    "        return self.params.get('timeout')\n",
    "\n",
    "    async def download(self, url_item):\n",
    "        url, method, data = url_item.get('url'), url_item.get(\n",
    "            'method'), url_item.get('data')\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = requests.get(url, headers=self.get_headers(),\n",
    "                                        proxies=self.get_proxy(), stream=True,\n",
    "                                        cookies=self.get_cookies(),\n",
    "                                        timeout=self.get_timeout())\n",
    "            else:\n",
    "                response = requests.post(url, headers=self.get_headers(),\n",
    "                                         proxies=self.get_proxy(), data=data,\n",
    "                                         stream=True,\n",
    "                                         cookies=self.get_cookies(),\n",
    "                                         timeout=self.get_timeout())\n",
    "            # 过滤content-length大于1M的html下载链接\n",
    "            if int(response.headers.get('Content-Length', 0)) < self.params.get('content_length'):\n",
    "                return response\n",
    "            else:\n",
    "                logging.warning('Content length > 1M, url: {}'.format(url))\n",
    "        except Exception as e:\n",
    "            logging.error('Download html error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "    async def get_response(self, url_item):\n",
    "        resp_list = []\n",
    "        response = await self.download(url_item)\n",
    "        if response:\n",
    "            resp = {}\n",
    "            resp['url_item'] = url_item\n",
    "            resp['response'] = response\n",
    "            resp_list.append(resp)\n",
    "            for history_item in response.history:\n",
    "                resp = {}\n",
    "                resp['url_item'] = url_item\n",
    "                resp['response'] = history_item\n",
    "                resp_list.append(resp)\n",
    "                self.history_num += 1\n",
    "        return resp_list\n",
    "\n",
    "\n",
    "class HtmlParser(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.manager = UrlManager(self.params)\n",
    "        # 所有不重复url的集合,包括不符合过滤条件的url,该集合仅用来过滤重复urls,并不是实际请求的urls数目\n",
    "        self.all_urls = set()\n",
    "        # 停止解析urls\n",
    "        self.stop_parse = False\n",
    "        # 被过滤掉的urls数目\n",
    "        self.all_passed_urls = 0\n",
    "        # 提取到的urls数目\n",
    "        self.all_abstract_urls = 0\n",
    "        self.parse_url_list = []\n",
    "\n",
    "    def filter_url(self, url_item):\n",
    "        url = url_item.get('url')\n",
    "        depth = url_item.get('depth')\n",
    "        amount = self.params.get('amount')\n",
    "        # 如果使用总量限制,且请求url超出总量设置,则过滤并停止提取url\n",
    "        if amount > 0 and self.all_abstract_urls > amount:\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url amount {} > {}, stop parse urls.'.format(\n",
    "                self.all_abstract_urls, self.params.get('amount')))\n",
    "            return True\n",
    "\n",
    "        # 支持url深度限制\n",
    "        if self.params.get('depth') > 0 and depth > self.params.get('depth'):\n",
    "            self.stop_parse = True\n",
    "            logging.info('Current url depth {} > {}, stop parse urls.'.format(\n",
    "                depth, self.params.get('depth')))\n",
    "            return True\n",
    "\n",
    "        # 支持域名过滤url\n",
    "        domain = urlparse(url).netloc\n",
    "        for allowed_domain in self.params.get('allowed_domains'):\n",
    "            if not fnmatch(domain, allowed_domain):\n",
    "                return True\n",
    "\n",
    "        # 支持排除关键字过滤url\n",
    "        for keyword in self.params.get('exclude_keywords'):\n",
    "            if keyword in url:\n",
    "                return True\n",
    "\n",
    "        # 过滤重复url\n",
    "        if url in self.all_urls:\n",
    "            return True\n",
    "        else:\n",
    "            self.all_urls.add(url)\n",
    "\n",
    "        return False\n",
    "\n",
    "    def parse_form_data(self, tag):\n",
    "        data = {}\n",
    "        for input in tag.find_all('input'):\n",
    "            name = input.get('name')\n",
    "            if name and input.get('type') in ['text', 'password']:\n",
    "                data[name] = input.get('value', '')\n",
    "            elif input.get('type') == 'submit':\n",
    "                name = 'submit'\n",
    "                data[name] = input.get('value', '')\n",
    "            else:\n",
    "                if data.get(name) is None:\n",
    "                    data[name] = list(input.get('value', ''))\n",
    "                else:\n",
    "                    data[name].append(input.get('value', ''))\n",
    "        return data\n",
    "\n",
    "    async def abstract_urls(self, response):\n",
    "        url_items = []\n",
    "        url_item = response.get('url_item')\n",
    "        resp = response.get('response')\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        tags = soup.find_all(True)\n",
    "        for tag in tags:\n",
    "            if self.stop_parse:\n",
    "                logging.info('Stop abstract urls.')\n",
    "                break\n",
    "            method, data = 'GET', None\n",
    "            if tag.name == 'form':\n",
    "                url = tag.get('action', '')\n",
    "                method = tag.get('method')\n",
    "                data = self.parse_form_data(tag)\n",
    "            elif tag.name == 'script':\n",
    "                url = tag.get('src', '')\n",
    "            else:\n",
    "                url = tag.get('href', '')\n",
    "            sub_url_item = self.manager.patch_url(\n",
    "                url, method=method, data=data, parent_url_obj=url_item)\n",
    "            if not self.filter_url(sub_url_item):\n",
    "                url_items.append(sub_url_item)\n",
    "                self.parse_url_list.append(sub_url_item.get('url'))\n",
    "            else:\n",
    "                self.all_passed_urls += 1\n",
    "        return url_items\n",
    "\n",
    "    async def parse(self, response):\n",
    "        url_items = []\n",
    "        if response and not self.stop_parse:\n",
    "            url_items = await self.abstract_urls(response)\n",
    "        self.all_abstract_urls += len(url_items)\n",
    "        return url_items\n",
    "\n",
    "\n",
    "class DataItem(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        db = connect_mongo(MONGODB)\n",
    "        self.movie = db['my_crawler_urls']\n",
    "        self.error_urls = 0\n",
    "        # 入库前再清洗一遍相同的url\n",
    "        self.res_urls = set()\n",
    "        self.duplicate_urls = 0\n",
    "\n",
    "    def handle_url(self, url):\n",
    "        url_obj = urlparse(url)\n",
    "        port = 443 if url_obj.scheme == 'https' else 80\n",
    "        if ':80' in url_obj.netloc or ':443' in url_obj.netloc:\n",
    "            new_netloc = url_obj.netloc\n",
    "        else:\n",
    "            new_netloc = '{}:{}'.format(url_obj.netloc, port)\n",
    "\n",
    "        if url_obj.params:\n",
    "            new_params = '?{}'.format(url_obj.params)\n",
    "        else:\n",
    "            new_params = url_obj.params\n",
    "\n",
    "        if url_obj.query:\n",
    "            new_query = '?{}'.format(url_obj.query)\n",
    "        else:\n",
    "            new_query = url_obj.query\n",
    "\n",
    "        new_url = '{}://{}{}{}{}{}'.format(url_obj.scheme, new_netloc,\n",
    "                                           url_obj.path, new_params,\n",
    "                                           new_query, url_obj.fragment)\n",
    "        return new_url\n",
    "\n",
    "    def handle_title(self, data):\n",
    "        title = ''\n",
    "        response = data.get('response')\n",
    "        html = response.content\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            title_tag = soup.title\n",
    "            title = title_tag.get_text() if title_tag else ''\n",
    "        return title\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        response = data.get('response')\n",
    "        request, resp = {}, {}\n",
    "        request['headers'] = response.request.headers\n",
    "        request['url'] = response.request.url\n",
    "        request['method'] = response.request.method\n",
    "\n",
    "        resp['headers'] = response.headers\n",
    "        resp['content'] = response.content\n",
    "        resp['status_code'] = response.status_code\n",
    "        url = self.handle_url(request['url'])\n",
    "        resp['url'] = url\n",
    "        if url not in self.res_urls:\n",
    "            self.res_urls.add(url)\n",
    "            return request, resp\n",
    "        else:\n",
    "            self.duplicate_urls += 1\n",
    "            return None\n",
    "\n",
    "    async def save(self, response):\n",
    "        doc = {}\n",
    "        url_item = response.get('url_item')\n",
    "        try:\n",
    "            data = self.handle_data(response)\n",
    "            if data:\n",
    "                request, resp = data[0], data[1]\n",
    "                title = self.handle_title(response)\n",
    "                doc['request'] = request\n",
    "                doc['response'] = resp\n",
    "                doc['title'] = title\n",
    "                doc['site'] = self.params.get('start_url')\n",
    "                doc['end_type'] = self.params.get('end_type')\n",
    "                doc['time'] = int(time.time() * 1000)\n",
    "                self.movie.insert(doc)\n",
    "        except Exception as e:\n",
    "            logging.error('Save data to mongodb error: {}'.format(e))\n",
    "            logging.error('Error url info: {}'.format(url_item))\n",
    "            self.error_urls += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_root_logger_settings()\n",
    "    spider = Spider(PARAMS)\n",
    "    spider.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05/08/2018 11:34:49 [INFO] [spiders.py] [199]: Spider started!\n",
    "05/08/2018 11:34:49 [INFO] [spiders.py] [165]: Start to create consume and produce tasks.\n",
    "05/08/2018 11:34:52 [INFO] [spiders.py] [336]: Current url amount 541 > 500, stop parse urls.\n",
    "05/08/2018 11:34:52 [INFO] [spiders.py] [389]: Stop abstract urls.\n",
    "05/08/2018 11:34:57 [ERROR] [spiders.py] [292]: Download html error: HTTPConnectionPool(host='pcdoodle.baidu.com', port=80): Max retries exceeded with url: /doodlebaike (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BF9238C7B8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))\n",
    "05/08/2018 11:34:57 [ERROR] [spiders.py] [293]: Error url info: {'base_url': 'https://www.baidu.com/s?wd=%E4%BB%8A%E6%97%A5%E6%96%B0%E9%B2%9C%E4%BA%8B&tn=SE_PclogoS_8whnvm25&sa=ire_dl_gh_logo&rsv_dl=igh_logo_pcs', 'depth': 3, 'url': 'http://www.baidu.com/link?url=MuTGTEGFr3teHcbL6WVBjAGg6N-DnpZr1ThrJuKFyI_xrPUcZCfH_9j_nxdJmJJr', 'domain': 'www.baidu.com', 'end_type': 'PC', 'method': 'GET', 'data': None}\n",
    "05/08/2018 11:35:59 [ERROR] [spiders.py] [496]: Save data to mongodb error: HTTPConnectionPool(host='shouji.baidu.com', port=80): Read timed out.\n",
    "05/08/2018 11:35:59 [ERROR] [spiders.py] [497]: Error url info: {'base_url': 'https://www.baidu.com/s?wd=%E7%99%BE%E5%BA%A6%E5%A5%BD%E7%9C%8B&tn=SE_PclogoS_8whnvm25&usm=3&ie=utf-8&rsv_cq=%E4%BB%8A%E6%97%A5%E6%96%B0%E9%B2%9C%E4%BA%8B&rsv_dl=0_right_recommends_merge_20826&euri=6b0794120dec11e68e38008cfaeb7e18', 'depth': 4, 'url': 'http://www.baidu.com/link?url=qU-7h5sRYKczMwPlGECptMaN1iFtVxw7dpQI-pBMIEIfsMDWI29kc42vFzbc7HQQ_F8Zi8a8GD2CFF6ug4UmYK', 'domain': 'www.baidu.com', 'end_type': 'PC', 'method': 'GET', 'data': None}\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [212]: Request num: 535.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [213]: Response num: 752.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [214]: History num: 218.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [215]: Abstract urls: 541.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [216]: Filters urls: 6421.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [217]: Error urls: 2.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [218]: All urls: 542.\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [219]: Time usage: 0:02:18.153237\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [220]: Spider finished!\n",
    "05/08/2018 11:37:07 [INFO] [spiders.py] [221]: Create 1341 tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
